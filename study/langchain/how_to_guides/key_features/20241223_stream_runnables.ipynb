{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an extensive summary in markdown format, drawing on the provided sources and our conversation history, with some bolding to highlight key concepts:\n",
    "\n",
    "**Streaming Runnables in LangChain**\n",
    "\n",
    "*   **Purpose**: This guide focuses on how to implement streaming in LangChain applications to make them feel more responsive to end-users. Streaming involves showing intermediate progress, such as output from an LLM token by token, rather than waiting for the entire response before displaying anything.\n",
    "\n",
    "*   **Key Concepts**:\n",
    "    *   **Runnable Interface**: Many LangChain primitives like chat models, output parsers, prompts, retrievers, and agents implement the LangChain Runnable Interface. This interface provides methods for streaming content.\n",
    "    *  **Input Streams**: Streaming is possible when all steps in a program can process input as a stream, handling chunks of input one at a time and yielding corresponding output chunks.\n",
    "\n",
    "*   **Streaming Methods**:\n",
    "    *   **`stream` and `astream`**: These methods are designed to stream the final output in chunks. `stream` is synchronous, while `astream` is asynchronous.\n",
    "    *  **`astream_events` and `astream_log`**: These asynchronous methods stream both intermediate steps and final output from the chain. The `astream_events` API is a beta feature, and may be subject to change.\n",
    "\n",
    "*   **Streaming with LLMs and Chat Models**:\n",
    "    *   LLMs are a primary bottleneck in LLM-based applications because they can take several seconds to generate a response, which is slower than the ~200-300 ms threshold for responsiveness.\n",
    "    *   Streaming the output from the model token by token is key to making applications feel more responsive.\n",
    "    *   The `stream` and `astream` methods yield `AIMessageChunk` objects, which represent a part of an `AIMessage`. These chunks can be added together to get the state of the response so far.\n",
    "\n",
    "*   **Streaming with Chains**:\n",
    "    *   Chains are constructed using the LangChain Expression Language (LCEL). LCEL chains automatically implement `stream` and `astream` for streaming the final output. LCEL also supports transform-style passthrough streaming, where primitives operate on each streaming chunk individually.\n",
    "    *   Components like prompt templates and chat models may interrupt the streaming process because they cannot process individual chunks, instead aggregating all previous steps.\n",
    "    *   Output parsers like `StrOutputParser` can be used to extract the content field from `AIMessageChunk` objects, giving the tokens returned by the model.\n",
    "    *   Custom functions can be written as generators, which allow them to operate on streams.\n",
    "\n",
    "*   **Working with Input Streams**:\n",
    "    *   Parsers need to operate on the input stream in order to stream results, such as JSON, as they are being generated, by attempting to \"auto-complete\" the partial JSON into a valid state. For example, a `JsonOutputParser` can incrementally output JSON as it becomes available.\n",
    "\n",
    "*   **Non-Streaming Components**:\n",
    "    *   Some components, such as retrievers, do not support streaming.\n",
    "    *   An LCEL chain that includes non-streaming components can still stream, with streaming of partial output beginning after the last non-streaming step.\n",
    "    *   However, steps in a chain that operate on finalized inputs rather than input streams can break streaming functionality via `stream` or `astream`.\n",
    "\n",
    "*   **Using Stream Events**:\n",
    "    *   The `astream_events` API streams intermediate steps, even if a chain contains steps that only operate on finalized inputs.\n",
    "    *   The API returns a variety of events from different components including `on_chat_model_start`, `on_chat_model_stream`, `on_chat_model_end`, `on_llm_start`, `on_llm_stream`, `on_llm_end`, `on_chain_start`, `on_chain_stream`, `on_chain_end`, `on_tool_start`, `on_tool_end`, `on_retriever_start`, `on_retriever_end`, `on_prompt_start`, and `on_prompt_end`.\n",
    "    *   Events include the event name, the data associated with the event (such as chunk, input, or output), the name of the component that emitted the event, any tags associated with the component and a run ID. The input to a runnable may not be known until after the input stream has been fully consumed.\n",
    "    *   Event streams can be filtered by component name, component type, or component tags.\n",
    "    *   When using tools or custom runnables, callbacks need to be propagated correctly to generate stream events.  Callbacks are passed automatically by `RunnableLambdas` or the `@chain` decorator.\n",
    "\n",
    "*   **Key Takeaways**\n",
    "    *   Streaming is a crucial technique for making LLM-based applications more responsive by showing intermediate progress.\n",
    "    *   LangChain's `Runnable` interface enables streaming through the `stream` and `astream` methods for final output, and the `astream_events` method for intermediate and final outputs.\n",
    "    *   LCEL facilitates declarative specification of chains with automatic streaming implementation.\n",
    "    *   Care must be taken to ensure that all steps in a chain can process input as a stream to avoid breaking the streaming functionality.\n",
    "    *   The `astream_events` API provides granular control over streaming outputs by emitting events from all the steps in a chain, and can be filtered by name, type or tag.\n",
    "\n",
    "This summary provides an overview of how to implement streaming in LangChain, drawing on the information provided in the sources and our conversation history, covering key concepts, methods, and options for constructing streaming applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "model = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| color| of| the| sky| can| vary| depending| on| several| factors|,| including| the| time| of| day|,| weather| conditions|,| and| atmospheric| particles|.| During| a| clear| day|,| the| sky| typically| appears| blue| due| to| the| scattering| of| sunlight| by| the| atmosphere|.| Near| sunrise| and| sunset|,| the| sky| can| take| on| shades| of| orange|,| pink|,| and| purple|.| When| it's| cloudy| or| over|cast|,| the| sky| may| look| gray|.| Additionally|,| pollution| and| other| atmospheric| conditions| can| affect| the| sky|'s| color|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "for chunk in model.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| answer| can| be| a| bit| tricky|,| as| it| depends| on| various| factors| such| as|:\n",
      "\n",
      "|1|.| **|Time| of| day|**:| During| the| daytime| (|around| noon|),| the| sky| appears| blue| due| to| a| phenomenon| called| Ray|leigh| scattering|,| where| shorter| wavelengths| like| blue| light| are| scattered| more| than| longer| wavelengths| like| red| light| by| the| tiny| molecules| of| gases| in| the| atmosphere|.\n",
      "|2|.| **|Weather| conditions|**:| On| cloudy| or| fog|gy| days|,| the| sky| can| appear| gray| or| white|.| During| sunrise| and| sunset|,| the| sky| takes| on| hues| of| orange|,| pink|,| and| purple| due| to| the| scattering| of| light| by| atmospheric| particles|.\n",
      "|3|.| **|Location| and| altitude|**:| The| color| of| the| sky| can| change| depending| on| your| location|'s| latitude|,| altitude|,| and| proximity| to| urban| areas| or| pollution| sources|.\n",
      "\n",
      "|So|,| to| answer| your| question|:| \"|The| color| of| the| sky| is|...| blue| (|during| daytime|),| but| it| changes| to| gray|/|white|/cloud|y| during| over|cast| conditions|,| and| orange|/p|ink|/p|urple| during| sunrise|/s|unset|!\"\n",
      "\n",
      "|Which| one| would| you| like| to| know| more| about|?||"
     ]
    }
   ],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "ollama = ChatOllama(model='llama3.1')\n",
    "chunks = []\n",
    "for chunk in ollama.stream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|The| color| of| the| sky| can| vary| depending| on| the| time| of| day|,| weather| conditions|,| and| atmospheric| factors|.| During| a| clear| day|,| the| sky| typically| appears| blue| due| to| the| scattering| of| sunlight| by| the| atmosphere|.| At| sunrise| and| sunset|,| the| sky| can| take| on| shades| of| orange|,| pink|,| and| red|.| When| it's| cloudy| or| over|cast|,| the| sky| may| appear| gray|.| At| night|,| the| sky| is| usually| dark|,| often| dotted| with| stars|.||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in model.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The| answer|,| of|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " course|,| depends| on| the| time| of| day| and| atmospheric| conditions|.\n",
      "\n",
      "|**|During| the| daytime|:|**\n",
      "|When| the| sun| is| overhead|,| the| sky| appears| blue| because| of| a| phenomenon| called| Ray|leigh| scattering|.| Short|er| (|blue|)| wavelengths| of| light| are| scattered| more| than| longer| (|red|)| wavelengths| by| the| tiny| molecules| of| gases| in| the| atmosphere|,| making| the| sky| appear| blue|.\n",
      "\n",
      "|**|At| sunrise| and| sunset|:|**\n",
      "|As| the| sun| rises| or| sets|,| the| sky| can| take| on| hues| of| red|,| orange|,| pink|,| and| purple| due| to| a| combination| of| atmospheric| scattering| and| dust| particles|.| This| is| because| the| light| has| to| travel| through| more| of| the| Earth|'s| atmosphere| to| reach| our| eyes|,| which| sc|atters| shorter| wavelengths| even| further|.\n",
      "\n",
      "|**|At| night|:|**\n",
      "|The| sky| appears| dark| or| black|,| but| with| some| exceptions|:\n",
      "\n",
      "|*| **|Moon|light|:**| The| full| moon| can| make| the| sky| appear| a| bright| gray|ish|-white|.\n",
      "|*| **|A|ur|ora| (|Northern|/S|ou|thern| Lights|):|**| During| high|-l|atitude| aur|or|ae| events|,| the| sky| can| display| vibrant| colors| like| green|,| blue|,| and| red|.\n",
      "\n",
      "|Keep| in| mind| that| these| are| general| descriptions|.| Atmospheric| conditions|,| pollution|,| and| other| factors| can| influence| the| color| of| the| sky|.\n",
      "\n",
      "|What| time| is| it| for| you|?| I|'d| be| happy| to| describe| what| your| sky| might| look| like|!||"
     ]
    }
   ],
   "source": [
    "chunks = []\n",
    "async for chunk in ollama.astream(\"what color is the sky?\"):\n",
    "    chunks.append(chunk)\n",
    "    print(chunk.content, end=\"|\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The', additional_kwargs={}, response_metadata={}, id='run-ec786023-8e4a-49cb-b010-9e13d8992188')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessageChunk(content='The answer, of course', additional_kwargs={}, response_metadata={}, id='run-ec786023-8e4a-49cb-b010-9e13d8992188')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chunks[0] + chunks[1] + chunks[2] + chunks[3] + chunks[4]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|Why| did| the| par|rot| wear| a| rain|coat|?\n",
      "\n",
      "|Because| it| wanted| to| be| a| poly|uns|aturated|!||"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here|'s| one|:\n",
      "\n",
      "|Why| did| the|"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " par|rot| go| to| the| doctor|?\n",
      "\n",
      "|Because| it| had| a| f|owl| cough|!| (|get| it|?)||"
     ]
    }
   ],
   "source": [
    "chain = prompt | ollama | parser\n",
    "\n",
    "async for chunk in chain.astream({\"topic\": \"parrot\"}):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 652}]}\n",
      "{'countries': [{'name': 'France', 'population': 652735}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 467}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 467547}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476}]}\n",
      "{'countries': [{'name': 'France', 'population': 65273511}, {'name': 'Spain', 'population': 46754778}, {'name': 'Japan', 'population': 126476461}]}\n"
     ]
    }
   ],
   "source": [
    "# the parser needs to operate on the input stream, and attempt to \"auto-complete\" the partial json into a valid state.\n",
    "\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n",
      "{'countries': []}\n",
      "{'countries': [{}]}\n",
      "{'countries': [{'name': ''}]}\n",
      "{'countries': [{'name': 'France'}]}\n",
      "{'countries': [{'name': 'France', 'population': 673}]}\n",
      "{'countries': [{'name': 'France', 'population': 673261}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 463}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 463331}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {'name': ''}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {'name': 'Japan'}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {'name': 'Japan', 'population': 127}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {'name': 'Japan', 'population': 127817}]}\n",
      "{'countries': [{'name': 'France', 'population': 67326100}, {'name': 'Spain', 'population': 46333111}, {'name': 'Japan', 'population': 127817000}]}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "chain = (\n",
    "    ollama | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import (\n",
    "    JsonOutputParser,\n",
    ")\n",
    "\n",
    "\n",
    "# A function that operates on finalized inputs\n",
    "# rather than on an input_stream\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\"\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "async def _extract_country_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    async for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "async for text in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "France|Spain|Japan|"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "\n",
    "def _extract_country_names_streaming(input_stream):\n",
    "    \"\"\"A function that operates on input streams.\"\"\"\n",
    "    country_names_so_far = set()\n",
    "\n",
    "    for input in input_stream:\n",
    "        if not isinstance(input, dict):\n",
    "            continue\n",
    "\n",
    "        if \"countries\" not in input:\n",
    "            continue\n",
    "\n",
    "        countries = input[\"countries\"]\n",
    "\n",
    "        if not isinstance(countries, list):\n",
    "            continue\n",
    "\n",
    "        for country in countries:\n",
    "            name = country.get(\"name\")\n",
    "            if not name:\n",
    "                continue\n",
    "            if name not in country_names_so_far:\n",
    "                yield name\n",
    "                country_names_so_far.add(name)\n",
    "\n",
    "\n",
    "chain = model | JsonOutputParser() | _extract_country_names_streaming\n",
    "\n",
    "for text in chain.stream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(text, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='f4cd12ea-80b4-4f6b-86cd-d869ffdf19f9', metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(id='1635956a-6dd6-459b-bfe0-4e49076191c8', metadata={}, page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "template = \"\"\"Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=OpenAIEmbeddings(),\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[Document(id='d2762d49-6554-49bc-b93c-0c98e73b4791', metadata={}, page_content='harrison worked at kensho'),\n",
       "  Document(id='cf7895f1-5442-4e07-93fd-e38cef944239', metadata={}, page_content='harrison likes spicy food')]]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_ollama import OllamaEmbeddings\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(model=\"snowflake-arctic-embed2\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(\n",
    "    [\"harrison worked at kensho\", \"harrison likes spicy food\"],\n",
    "    embedding=ollama_emb,\n",
    ")\n",
    "retriever = vectorstore.as_retriever(search_kwargs={'k': 3})\n",
    "\n",
    "chunks = [chunk for chunk in retriever.stream(\"where did harrison work?\")]\n",
    "chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "retrieval_chain = (\n",
    "    {\n",
    "        \"context\": retriever.with_config(run_name=\"Docs\"),\n",
    "        \"question\": RunnablePassthrough(),\n",
    "    }\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|H|arrison| worked| at| Kens|ho|.| Kens|ho| is| known| for| its| innovative| approach| to| data| analysis|,| blending| cutting|-edge| technology| with| deep| market| insights|.| The| team| at| Kens|ho| often| collabor|ates| on| projects| that| require| both| creativity| and| analytical| rigor|,| creating| a| dynamic| work| environment|.| Employees| frequently| participate| in| brainstorming| sessions| that| encourage| out|-of|-the|-box| thinking|,| making| it| a| hub| for| forward|-thinking| professionals|.||"
     ]
    }
   ],
   "source": [
    "for chunk in retrieval_chain.stream(\n",
    "    \"Where did harrison work? \" \"Write 3 made up sentences about this place.\"\n",
    "):\n",
    "    print(chunk, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "events = []\n",
    "async for event in model.astream_events(\"hello\", version=\"v2\"):\n",
    "    events.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_start',\n",
       "  'data': {'input': 'hello'},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'run_id': '0858c65b-a131-4175-b174-53898ed99c6a',\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '0858c65b-a131-4175-b174-53898ed99c6a',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-0858c65b-a131-4175-b174-53898ed99c6a')},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'run_id': '0858c65b-a131-4175-b174-53898ed99c6a',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'data': {'chunk': AIMessageChunk(content='Hello', additional_kwargs={}, response_metadata={}, id='run-0858c65b-a131-4175-b174-53898ed99c6a')},\n",
       "  'parent_ids': []}]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chat_model_stream',\n",
       "  'run_id': '0858c65b-a131-4175-b174-53898ed99c6a',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b'}, id='run-0858c65b-a131-4175-b174-53898ed99c6a')},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_end',\n",
       "  'data': {'output': AIMessageChunk(content='Hello! How can I assist you today?', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b'}, id='run-0858c65b-a131-4175-b174-53898ed99c6a')},\n",
       "  'run_id': '0858c65b-a131-4175-b174-53898ed99c6a',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': [],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'parent_ids': []}]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = (\n",
    "    model | JsonOutputParser()\n",
    ")  # Due to a bug in older versions of Langchain, JsonOutputParser did not stream results from some models\n",
    "\n",
    "events = [\n",
    "    event\n",
    "    async for event in chain.astream_events(\n",
    "        \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "        'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "        \"Each country should have the key `name` and `population`\",\n",
    "        version=\"v2\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_chain_start',\n",
       "  'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'},\n",
       "  'name': 'RunnableSequence',\n",
       "  'tags': [],\n",
       "  'run_id': 'f3a9d910-6526-49be-a81f-b77553425bf0',\n",
       "  'metadata': {},\n",
       "  'parent_ids': []},\n",
       " {'event': 'on_chat_model_start',\n",
       "  'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}},\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'run_id': '01fae1c4-6b42-4232-9f55-89fcf19fe86e',\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'parent_ids': ['f3a9d910-6526-49be-a81f-b77553425bf0']},\n",
       " {'event': 'on_chat_model_stream',\n",
       "  'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-01fae1c4-6b42-4232-9f55-89fcf19fe86e')},\n",
       "  'run_id': '01fae1c4-6b42-4232-9f55-89fcf19fe86e',\n",
       "  'name': 'ChatOpenAI',\n",
       "  'tags': ['seq:step:1'],\n",
       "  'metadata': {'ls_provider': 'openai',\n",
       "   'ls_model_name': 'gpt-4o-mini',\n",
       "   'ls_model_type': 'chat',\n",
       "   'ls_temperature': 0.7},\n",
       "  'parent_ids': ['f3a9d910-6526-49be-a81f-b77553425bf0']}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'event': 'on_parser_end',\n",
       "  'data': {'output': {'countries': [{'name': 'France', 'population': 65273511},\n",
       "     {'name': 'Spain', 'population': 46754778},\n",
       "     {'name': 'Japan', 'population': 126476461}]},\n",
       "   'input': AIMessageChunk(content='Here is the JSON representation of the countries France, Spain, and Japan along with their populations:\\n\\n```json\\n{\\n  \"countries\": [\\n    {\\n      \"name\": \"France\",\\n      \"population\": 65273511\\n    },\\n    {\\n      \"name\": \"Spain\",\\n      \"population\": 46754778\\n    },\\n    {\\n      \"name\": \"Japan\",\\n      \"population\": 126476461\\n    }\\n  ]\\n}\\n```\\n\\nPlease note that the population figures are based on estimates as of 2021, and actual numbers may vary.', additional_kwargs={}, response_metadata={'finish_reason': 'stop', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_0aa8d3e20b'}, id='run-01fae1c4-6b42-4232-9f55-89fcf19fe86e')},\n",
       "  'run_id': 'ea3ec37e-e9e4-4bec-aa6a-743859a06451',\n",
       "  'name': 'JsonOutputParser',\n",
       "  'tags': ['seq:step:2'],\n",
       "  'metadata': {},\n",
       "  'parent_ids': ['f3a9d910-6526-49be-a81f-b77553425bf0']},\n",
       " {'event': 'on_chain_end',\n",
       "  'data': {'output': {'countries': [{'name': 'France', 'population': 65273511},\n",
       "     {'name': 'Spain', 'population': 46754778},\n",
       "     {'name': 'Japan', 'population': 126476461}]}},\n",
       "  'run_id': 'f3a9d910-6526-49be-a81f-b77553425bf0',\n",
       "  'name': 'RunnableSequence',\n",
       "  'tags': [],\n",
       "  'metadata': {},\n",
       "  'parent_ids': []}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "events[-2:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: ''\n",
      "Chat model chunk: 'Here'\n",
      "Chat model chunk: ' is'\n",
      "Chat model chunk: ' the'\n",
      "Chat model chunk: ' JSON'\n",
      "Chat model chunk: ' representation'\n",
      "Chat model chunk: ' of'\n",
      "Chat model chunk: ' the'\n",
      "Chat model chunk: ' countries'\n",
      "Chat model chunk: ' France'\n",
      "Chat model chunk: ','\n",
      "Chat model chunk: ' Spain'\n",
      "Chat model chunk: ','\n",
      "Chat model chunk: ' and'\n",
      "Chat model chunk: ' Japan'\n",
      "Chat model chunk: ' along'\n",
      "Chat model chunk: ' with'\n",
      "Chat model chunk: ' their'\n",
      "Chat model chunk: ' populations'\n",
      "Chat model chunk: ':\\n\\n'\n",
      "Chat model chunk: '```'\n",
      "Chat model chunk: 'json'\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '{\\n'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: ' '\n",
      "Chat model chunk: ' \"'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_parser_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'my_parser', 'tags': ['seq:step:2'], 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'metadata': {}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': []}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': ''}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France'}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 652}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 652735}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 65273511}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 65273511}, {}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "{'event': 'on_parser_stream', 'run_id': '91df3da0-9174-4384-82ce-44e895661de8', 'name': 'my_parser', 'tags': ['seq:step:2'], 'metadata': {}, 'data': {'chunk': {'countries': [{'name': 'France', 'population': 65273511}, {'name': ''}]}}, 'parent_ids': ['a6d5985d-5e6c-49c2-846d-b9ecf5b812a2']}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "    include_names=[\"my_parser\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chat_model_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'model', 'tags': ['seq:step:1'], 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Here', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' requested', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' information', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' in', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' JSON', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' format', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=':\\n\\n', additional_kwargs={}, response_metadata={}, id='run-0dd36d4b-a3fd-409c-b69b-8c9f3bb82260')}, 'run_id': '0dd36d4b-a3fd-409c-b69b-8c9f3bb82260', 'name': 'model', 'tags': ['seq:step:1'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['05cc7f48-b5c9-463c-b9b9-a145ea51f811']}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = model.with_config({\"run_name\": \"model\"}) | JsonOutputParser().with_config(\n",
    "    {\"run_name\": \"my_parser\"}\n",
    ")\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v2\",\n",
    "    include_types=[\"chat_model\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': 'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`'}, 'name': 'RunnableSequence', 'tags': ['my_chain'], 'run_id': '34742eb5-82bc-450c-b619-aa22d743c78c', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chat_model_start', 'data': {'input': {'messages': [[HumanMessage(content='output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`', additional_kwargs={}, response_metadata={})]]}}, 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_parser_start', 'data': {}, 'name': 'JsonOutputParser', 'tags': ['seq:step:2', 'my_chain'], 'run_id': '4d3e1670-861c-4bb1-b289-2c482d100e8b', 'metadata': {}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content='Here', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' is', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' JSON', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' format', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' list', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "{'event': 'on_chat_model_stream', 'data': {'chunk': AIMessageChunk(content=' of', additional_kwargs={}, response_metadata={}, id='run-a0e02020-78e5-4fb0-823d-c2af9a2d1a56')}, 'run_id': 'a0e02020-78e5-4fb0-823d-c2af9a2d1a56', 'name': 'ChatOpenAI', 'tags': ['seq:step:1', 'my_chain'], 'metadata': {'ls_provider': 'openai', 'ls_model_name': 'gpt-4o-mini', 'ls_model_type': 'chat', 'ls_temperature': 0.7}, 'parent_ids': ['34742eb5-82bc-450c-b619-aa22d743c78c']}\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "chain = (model | JsonOutputParser()).with_config({\"tags\": [\"my_chain\"]})\n",
    "\n",
    "max_events = 0\n",
    "async for event in chain.astream_events(\n",
    "    'output a list of the countries france, spain and japan and their populations in JSON format. Use a dict with an outer key of \"countries\" which contains a list of countries. Each country should have the key `name` and `population`',\n",
    "    version=\"v2\",\n",
    "    include_tags=[\"my_chain\"],\n",
    "):\n",
    "    print(event)\n",
    "    max_events += 1\n",
    "    if max_events > 10:\n",
    "        # Truncate output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that does not support streaming.\n",
    "# It operates on the finalizes inputs rather than\n",
    "# operating on the input stream.\n",
    "def _extract_country_names(inputs):\n",
    "    \"\"\"A function that does not operates on input streams and breaks streaming.\"\"\"\n",
    "    if not isinstance(inputs, dict):\n",
    "        return \"\"\n",
    "\n",
    "    if \"countries\" not in inputs:\n",
    "        return \"\"\n",
    "\n",
    "    countries = inputs[\"countries\"]\n",
    "\n",
    "    if not isinstance(countries, list):\n",
    "        return \"\"\n",
    "\n",
    "    country_names = [\n",
    "        country.get(\"name\") for country in countries if isinstance(country, dict)\n",
    "    ]\n",
    "    return country_names\n",
    "\n",
    "\n",
    "chain = (\n",
    "    model | JsonOutputParser() | _extract_country_names\n",
    ")  # This parser only works with OpenAI right now\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['France', 'Spain', 'Japan']\n"
     ]
    }
   ],
   "source": [
    "async for chunk in chain.astream(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "):\n",
    "    print(chunk, flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat model chunk: ''\n",
      "Chat model chunk: 'Here'\n",
      "Chat model chunk: ' is'\n",
      "Chat model chunk: ' the'\n",
      "Chat model chunk: ' JSON'\n",
      "Chat model chunk: ' representation'\n",
      "Chat model chunk: ' of'\n",
      "Chat model chunk: ' the'\n",
      "Chat model chunk: ' countries'\n",
      "Chat model chunk: ' France'\n",
      "Chat model chunk: ','\n",
      "Chat model chunk: ' Spain'\n",
      "Chat model chunk: ','\n",
      "Chat model chunk: ' and'\n",
      "Chat model chunk: ' Japan'\n",
      "Chat model chunk: ' along'\n",
      "Chat model chunk: ' with'\n",
      "Chat model chunk: ' their'\n",
      "Chat model chunk: ' populations'\n",
      "Chat model chunk: ':\\n\\n'\n",
      "Chat model chunk: '```'\n",
      "Chat model chunk: 'json'\n",
      "Chat model chunk: '\\n'\n",
      "Chat model chunk: '{\\n'\n",
      "Parser chunk: {}\n",
      "Chat model chunk: ' '\n",
      "Chat model chunk: ' \"'\n",
      "...\n"
     ]
    }
   ],
   "source": [
    "num_events = 0\n",
    "\n",
    "async for event in chain.astream_events(\n",
    "    \"output a list of the countries france, spain and japan and their populations in JSON format. \"\n",
    "    'Use a dict with an outer key of \"countries\" which contains a list of countries. '\n",
    "    \"Each country should have the key `name` and `population`\",\n",
    "    version=\"v2\",\n",
    "):\n",
    "    kind = event[\"event\"]\n",
    "    if kind == \"on_chat_model_stream\":\n",
    "        print(\n",
    "            f\"Chat model chunk: {repr(event['data']['chunk'].content)}\",\n",
    "            flush=True,\n",
    "        )\n",
    "    if kind == \"on_parser_stream\":\n",
    "        print(f\"Parser chunk: {event['data']['chunk']}\", flush=True)\n",
    "    num_events += 1\n",
    "    if num_events > 30:\n",
    "        # Truncate the output\n",
    "        print(\"...\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'bad_tool', 'tags': [], 'run_id': 'a2c37a3b-49f8-41bc-a1ac-692b7d4126dd', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'fb9b419a-8333-4077-907a-9efd327a295d', 'metadata': {}, 'parent_ids': ['a2c37a3b-49f8-41bc-a1ac-692b7d4126dd']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': 'fb9b419a-8333-4077-907a-9efd327a295d', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['a2c37a3b-49f8-41bc-a1ac-692b7d4126dd']}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': 'a2c37a3b-49f8-41bc-a1ac-692b7d4126dd', 'name': 'bad_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tools import tool\n",
    "\n",
    "\n",
    "def reverse_word(word: str):\n",
    "    return word[::-1]\n",
    "\n",
    "\n",
    "reverse_word = RunnableLambda(reverse_word)\n",
    "\n",
    "\n",
    "@tool\n",
    "def bad_tool(word: str):\n",
    "    \"\"\"Custom tool that doesn't propagate callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word)\n",
    "\n",
    "\n",
    "async for event in bad_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_tool_start', 'data': {'input': 'hello'}, 'name': 'correct_tool', 'tags': [], 'run_id': '17425d62-e534-46a2-91f9-374a4beb5125', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': 'hello'}, 'name': 'reverse_word', 'tags': [], 'run_id': '4a7ce7f2-b5f4-4881-9df9-f80b776d9076', 'metadata': {}, 'parent_ids': ['17425d62-e534-46a2-91f9-374a4beb5125']}\n",
      "{'event': 'on_chain_end', 'data': {'output': 'olleh', 'input': 'hello'}, 'run_id': '4a7ce7f2-b5f4-4881-9df9-f80b776d9076', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['17425d62-e534-46a2-91f9-374a4beb5125']}\n",
      "{'event': 'on_tool_end', 'data': {'output': 'olleh'}, 'run_id': '17425d62-e534-46a2-91f9-374a4beb5125', 'name': 'correct_tool', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "@tool\n",
    "def correct_tool(word: str, callbacks):\n",
    "    \"\"\"A tool that correctly propagates callbacks.\"\"\"\n",
    "    return reverse_word.invoke(word, {\"callbacks\": callbacks})\n",
    "\n",
    "\n",
    "async for event in correct_tool.astream_events(\"hello\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': 'e14b3302-9b61-4e5d-abfb-a350c077955d', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'c5e41cbf-e127-4dab-b3bb-d8a501ce5f68', 'metadata': {}, 'parent_ids': ['e14b3302-9b61-4e5d-abfb-a350c077955d']}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': 'c5e41cbf-e127-4dab-b3bb-d8a501ce5f68', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['e14b3302-9b61-4e5d-abfb-a350c077955d']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'e14b3302-9b61-4e5d-abfb-a350c077955d', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': 'e14b3302-9b61-4e5d-abfb-a350c077955d', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "reverse_and_double = RunnableLambda(reverse_and_double)\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_and_double', 'tags': [], 'run_id': 'c46bc257-6003-4b5a-9ba3-06e17908b3ac', 'metadata': {}, 'parent_ids': []}\n",
      "{'event': 'on_chain_start', 'data': {'input': '1234'}, 'name': 'reverse_word', 'tags': [], 'run_id': 'dd021df8-2b1d-4d15-baec-ae6cd825eca3', 'metadata': {}, 'parent_ids': ['c46bc257-6003-4b5a-9ba3-06e17908b3ac']}\n",
      "{'event': 'on_chain_end', 'data': {'output': '4321', 'input': '1234'}, 'run_id': 'dd021df8-2b1d-4d15-baec-ae6cd825eca3', 'name': 'reverse_word', 'tags': [], 'metadata': {}, 'parent_ids': ['c46bc257-6003-4b5a-9ba3-06e17908b3ac']}\n",
      "{'event': 'on_chain_stream', 'run_id': 'c46bc257-6003-4b5a-9ba3-06e17908b3ac', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'data': {'chunk': '43214321'}, 'parent_ids': []}\n",
      "{'event': 'on_chain_end', 'data': {'output': '43214321'}, 'run_id': 'c46bc257-6003-4b5a-9ba3-06e17908b3ac', 'name': 'reverse_and_double', 'tags': [], 'metadata': {}, 'parent_ids': []}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import chain\n",
    "\n",
    "\n",
    "@chain\n",
    "async def reverse_and_double(word: str):\n",
    "    return await reverse_word.ainvoke(word) * 2\n",
    "\n",
    "\n",
    "await reverse_and_double.ainvoke(\"1234\")\n",
    "\n",
    "async for event in reverse_and_double.astream_events(\"1234\", version=\"v2\"):\n",
    "    print(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
