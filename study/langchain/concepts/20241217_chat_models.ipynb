{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://python.langchain.com/docs/concepts/chat_models/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the provided document on chat models, formatted in markdown:\n",
    "\n",
    "**Overview**\n",
    "\n",
    "*   **Large Language Models (LLMs)** are advanced machine learning models adept at various language tasks like text generation, translation, and question answering.\n",
    "*   Modern LLMs are typically accessed through a **chat model interface** which takes a list of messages as input and returns a message as output.\n",
    "*   The newest chat models offer **tool calling, structured output, and multimodality** capabilities.\n",
    "\n",
    "**Features**\n",
    "\n",
    "*   LangChain provides a consistent interface for working with various chat models, along with monitoring, debugging, and optimisation features.\n",
    "*   It integrates with many providers, including **Anthropic, OpenAI, Ollama, Microsoft Azure, Google Vertex, Amazon Bedrock, Hugging Face, Cohere, and Groq**.\n",
    "*   LangChain uses its own message format or **OpenAI's message format**.\n",
    "*   It offers a **standard tool calling API**, a standard API for structuring outputs, async programming, efficient batching, and a rich streaming API.\n",
    "*   It also integrates with **LangSmith** for monitoring and debugging and offers features like standardized token usage, rate limiting, and caching.\n",
    "*   Chat models in LangChain are typically named with a \"Chat\" prefix, such as `ChatOllama`, `ChatAnthropic`, and `ChatOpenAI`. Models without this prefix are usually older models that use a string-in, string-out interface.\n",
    "\n",
    "**Integrations**\n",
    "\n",
    "*   LangChain offers integrations with both **official models**, supported by LangChain or the provider, and **community models**, contributed by the community.\n",
    "*   Official models are found in `langchain-<provider>` packages, while community models are in the `langchain-community` package.\n",
    "\n",
    "**Interface**\n",
    "\n",
    "*   LangChain chat models implement the `BaseChatModel` interface, which also implements the `Runnable` interface.\n",
    "*   Key methods operate on messages as input and return messages as output.\n",
    "*   Standard parameters configure the model's behaviour, such as **temperature, maximum tokens, and timeout**.\n",
    "*   Older LLMs without the \"Chat\" prefix use a string-in, string-out interface and are not recommended for general use.\n",
    "*   **Key Methods:**\n",
    "    *   `invoke`: Primary method for interacting with a chat model.\n",
    "    *   `stream`: Streams the output as it's generated.\n",
    "    *   `batch`: Batches multiple requests for efficient processing.\n",
    "    *   `bind_tools`: Binds tools to the model's execution context.\n",
    "    *    `with_structured_output`: Wraps the `invoke` method for models that support structured output.\n",
    "\n",
    "**Inputs and Outputs**\n",
    "\n",
    "*   Chat models use messages with associated roles (e.g., system, human, assistant) and content blocks that can include text or multimodal data.\n",
    "*   LangChain supports both its own message format and OpenAI's message format.\n",
    "\n",
    "**Standard Parameters**\n",
    "\n",
    "*   Standard parameters include:\n",
    "    *   `model`: Name of the specific AI model.\n",
    "    *   `temperature`: Controls randomness in the output.\n",
    "    *   `timeout`: Maximum time to wait for a response.\n",
    "    *   `max_tokens`: Limits the number of tokens in the response.\n",
    "    *   `stop`: Specifies sequences that stop token generation.\n",
    "    *    `max_retries`: Maximum number of retries on request failure.\n",
    "    *   `api_key`: API key for authentication.\n",
    "    *   `base_url`: API endpoint URL.\n",
    "    *   `rate_limiter`: Spacing out requests to avoid exceeding rate limits.\n",
    "*   Standard parameters are only supported on integrations with their own packages (e.g. `langchain-openai`) and not on models in `langchain-community`.\n",
    "*   Specific integrations may have additional parameters.\n",
    "\n",
    "**Tool Calling**\n",
    "\n",
    "*   Chat models can call tools to perform tasks such as fetching data or making API requests.\n",
    "\n",
    "**Structured Outputs**\n",
    "\n",
    "*   Chat models can respond in specific formats, like JSON, which is useful for information extraction.\n",
    "\n",
    "**Multimodality**\n",
    "\n",
    "*   LLMs can process data such as images, audio, and video in addition to text.\n",
    "*   Currently, only a few LLMs support multimodal inputs, and very few support multimodal outputs.\n",
    "\n",
    "**Context Window**\n",
    "\n",
    "*   The context window is the maximum size of the input sequence a model can process at once.\n",
    "*   Exceeding the context window can cause errors, which is especially important in conversational applications where the model needs to \"remember\" the context.\n",
    "*  Input size is measured in tokens\n",
    "\n",
    "**Rate Limiting**\n",
    "\n",
    "*   Chat model providers often impose limits on the number of requests made in a time period.\n",
    "*   Rate limit errors can be handled by spacing out requests, retrying, or falling back to another model.\n",
    "*   The `rate_limiter` parameter can be used to control request rates.\n",
    "\n",
    "**Caching**\n",
    "\n",
    "*   Caching can improve performance by reducing requests to the model provider but is complex in practice.\n",
    "*   Semantic caching, where responses are cached based on the meaning of the input, can be an alternative approach.\n",
    "*   Caching can be beneficial for frequently asked questions.\n",
    "\n",
    "**Related Resources**\n",
    "\n",
    "*   How-to guides on using chat models are available.\n",
    "*   A list of supported chat models can be found in the integrations section.\n",
    "*   Conceptual guides include messages, tool calling, multimodality, structured outputs, and tokens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.messages import HumanMessage, ToolMessage\n",
    "from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain_core.tools import tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.2.1'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import langchain_ollama \n",
    "langchain_ollama.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChatOllama(model=\"llama3.1\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INVOKE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a language model, so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T17:32:47.959824224Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1618173794, 'load_duration': 1320412509, 'prompt_eval_count': 16, 'prompt_eval_duration': 57000000, 'eval_count': 33, 'eval_duration': 238000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-b1ee33c6-e68d-4577-9e08-f0823e7cb54a-0', usage_metadata={'input_tokens': 16, 'output_tokens': 33, 'total_tokens': 49})"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke([HumanMessage(\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. But thank you for asking! How can I help you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T17:04:57.407448811Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1681449363, 'load_duration': 1315520398, 'prompt_eval_count': 16, 'prompt_eval_duration': 83000000, 'eval_count': 33, 'eval_duration': 280000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-b91d94db-48b8-48d8-9413-6dd7b8652f48-0', usage_metadata={'input_tokens': 16, 'output_tokens': 33, 'total_tokens': 49}),\n",
       " AIMessage(content='Here\\'s a simple function that checks if a number is odd:\\n\\n```python\\ndef is_odd(n):\\n    \"\"\"\\n    Checks if a number is odd.\\n\\n    Args:\\n        n (int): The number to check.\\n\\n    Returns:\\n        bool: True if the number is odd, False otherwise.\\n    \"\"\"\\n    return n % 2 != 0\\n```\\n\\nYou can use it like this:\\n\\n```python\\nprint(is_odd(5))   # Output: True\\nprint(is_odd(4))   # Output: False\\n```\\n\\nThis function uses the modulus operator (`%`) to find the remainder of `n` divided by `2`. If the remainder is not zero, then the number is odd.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T17:04:58.233771949Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2508347092, 'load_duration': 1315670269, 'prompt_eval_count': 26, 'prompt_eval_duration': 59000000, 'eval_count': 146, 'eval_duration': 1131000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-547898d6-d930-4961-83c6-c16308b8722b-0', usage_metadata={'input_tokens': 26, 'output_tokens': 146, 'total_tokens': 172})]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.batch([\n",
    "    [HumanMessage(\"Hello, how are you?\")],\n",
    "    [HumanMessage(\"Can you implement me a function in python which find if a number is odd?\")]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 I'm just a computer program, so I don't have feelings or emotions like humans do. I'm functioning properly and ready to help with any questions or tasks you may have! How about you? How's your day going so far?\n",
      "1 Here's a simple implementation of a function that checks whether a given number is odd or not.\n",
      "\n",
      "```python\n",
      "def is_odd(number):\n",
      "    \"\"\"\n",
      "    Checks whether the given number is odd or not.\n",
      "    \n",
      "    Args:\n",
      "        number (int): The number to check for parity.\n",
      "    \n",
      "    Returns:\n",
      "        bool: True if the number is odd, False otherwise.\n",
      "    \"\"\"\n",
      "    return number % 2 != 0\n",
      "\n",
      "# Example usage\n",
      "print(is_odd(5))   # True\n",
      "print(is_odd(4))   # False\n",
      "```\n",
      "\n",
      "This function works by using Python's modulus operator (`%`). When you use `a % b`, it returns the remainder of dividing `a` by `b`. So, in this case, if a number is odd, it can't be divided evenly by 2 (i.e., there will always be a remainder). If the remainder when the number is divided by 2 is not zero, then the number is indeed odd.\n"
     ]
    }
   ],
   "source": [
    "for i, message in model.batch_as_completed([\n",
    "    [HumanMessage(\"Hello, how are you?\")],\n",
    "    [HumanMessage(\"Can you implement me a function in python which find if a number is odd?\")]\n",
    "]):\n",
    "    print(i, message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm just a computer program, so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today?"
     ]
    }
   ],
   "source": [
    "for message in model.stream([HumanMessage(\"Hello, how are you?\")]):\n",
    "    print(message.content, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STRUCTURED OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Output(feelings=['happy'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Output(BaseModel):\n",
    "    feelings: List[str]\n",
    "\n",
    "model.with_structured_output(Output).invoke([HumanMessage(\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BIND TOOLS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T16:40:43.051233782Z', 'done': True, 'done_reason': 'stop', 'total_duration': 160531677, 'load_duration': 13436325, 'prompt_eval_count': 163, 'prompt_eval_duration': 13000000, 'eval_count': 17, 'eval_duration': 133000000, 'message': Message(role='assistant', content='', images=None, tool_calls=[ToolCall(function=Function(name='get_weather', arguments={'city': 'Paris'}))])}, id='run-da855911-1cb3-4b45-9785-604743414b6b-0', tool_calls=[{'name': 'get_weather', 'args': {'city': 'Paris'}, 'id': 'a75b2ffc-df82-44d5-b5a6-479ce3b356e9', 'type': 'tool_call'}], usage_metadata={'input_tokens': 163, 'output_tokens': 17, 'total_tokens': 180})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tool\n",
    "def get_weather(city: str) -> str:\n",
    "    \"\"\" Get the weather of a city \"\"\"\n",
    "    return \"Weather is super hot in \" + city + \" 35C and windy\"\n",
    "\n",
    "\n",
    "tooled_model = model.bind_tools([get_weather])\n",
    "response = tooled_model.invoke([HumanMessage(\"Hello, how is the weather in Paris?\")])\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='Weather is super hot in Paris 35C and windy' tool_call_id='a75b2ffc-df82-44d5-b5a6-479ce3b356e9'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"It sounds like you're experiencing a heatwave in Paris! A temperature of 35째C (95째F) can be quite challenging, especially when combined with wind.\\n\\nTo stay cool and comfortable, consider the following tips:\\n\\n1. **Stay hydrated**: Drink plenty of water to help your body regulate its temperature.\\n2. **Dress accordingly**: Wear lightweight, light-colored clothing that allows for good airflow and helps reflect the sun's rays.\\n3. **Take breaks in cooler spaces**: Whenever possible, seek shade or air-conditioned areas to give your body a break from the heat.\\n4. **Avoid strenuous activities**: Try to limit physical activity to early morning or evening when the temperature is lower.\\n\\nRemember to check the weather forecast regularly for any updates on the heatwave and wind conditions.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T16:45:03.884017873Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1208948463, 'load_duration': 14881829, 'prompt_eval_count': 69, 'prompt_eval_duration': 10000000, 'eval_count': 160, 'eval_duration': 1182000000, 'message': Message(role='assistant', content=\"It sounds like you're experiencing a heatwave in Paris! A temperature of 35째C (95째F) can be quite challenging, especially when combined with wind.\\n\\nTo stay cool and comfortable, consider the following tips:\\n\\n1. **Stay hydrated**: Drink plenty of water to help your body regulate its temperature.\\n2. **Dress accordingly**: Wear lightweight, light-colored clothing that allows for good airflow and helps reflect the sun's rays.\\n3. **Take breaks in cooler spaces**: Whenever possible, seek shade or air-conditioned areas to give your body a break from the heat.\\n4. **Avoid strenuous activities**: Try to limit physical activity to early morning or evening when the temperature is lower.\\n\\nRemember to check the weather forecast regularly for any updates on the heatwave and wind conditions.\", images=None, tool_calls=None)}, id='run-9d0806ca-6158-45ad-87c9-408db997a445-0', usage_metadata={'input_tokens': 69, 'output_tokens': 160, 'total_tokens': 229})"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if response.tool_calls and response.tool_calls[0]['name'] == 'get_weather':\n",
    "    tool_call = response.tool_calls[0]\n",
    "    tool_output = get_weather.invoke(tool_call['args'])  # Execute the tool with the provided arguments\n",
    "\n",
    "    # Create a ToolMessage with the tool's output\n",
    "    tool_message = ToolMessage(content=tool_output, tool_call_id=tool_call['id'])\n",
    "\n",
    "print(tool_message)\n",
    "\n",
    "tooled_model.invoke([tool_message])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ToolMessage(content='Weather is super hot in Paris 35C and windy', tool_call_id='a75b2ffc-df82-44d5-b5a6-479ce3b356e9')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tool_message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Based on the output from the `get_weather` tool call, I can tell you that it's currently very hot (35C) and windy in Paris.\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T16:45:13.396748605Z', 'done': True, 'done_reason': 'stop', 'total_duration': 271283207, 'load_duration': 23222240, 'prompt_eval_count': 103, 'prompt_eval_duration': 2000000, 'eval_count': 33, 'eval_duration': 245000000, 'message': Message(role='assistant', content=\"Based on the output from the `get_weather` tool call, I can tell you that it's currently very hot (35C) and windy in Paris.\", images=None, tool_calls=None)}, id='run-2862d00e-6d14-41cf-a800-2b5fe0b8e530-0', usage_metadata={'input_tokens': 103, 'output_tokens': 33, 'total_tokens': 136})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tooled_model.invoke([\n",
    "    HumanMessage(\"Hello, how is the weather in Paris?\"),\n",
    "    response,\n",
    "    tool_message\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
