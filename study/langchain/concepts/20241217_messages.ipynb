{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model Messages: An Overview\n",
    "\n",
    "This document details how messages are structured and used within chat models, particularly focusing on how LangChain handles them.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "*   **Messages** are the basic units of communication in chat models, representing both inputs and outputs.\n",
    "*   Each message has a **`role`** and **`content`**, along with optional metadata.\n",
    "*   **LangChain** provides a consistent way to manage messages across different chat models.\n",
    "\n",
    "### Message Components\n",
    "\n",
    "*   **`Role`**:  This categorises the message and helps the chat model respond appropriately. Key roles include:\n",
    "    *   **`system`**: Used to instruct the model and provide context.\n",
    "    *   **`user`**: Represents input from a user.\n",
    "    *   **`assistant`**: Represents a response from the model.\n",
    "    *   **`tool`**: Passes results of a tool invocation back to the model.\n",
    "    *   `function`: A legacy role for OpenAI's function-calling API, `tool` should be used instead.\n",
    "*   **`Content`**: The actual message information, can be text or multimodal data (images, audio, video). Most models primarily use text.\n",
    "*   **Other Data**: Messages may include:\n",
    "    *   **`ID`**: A unique identifier.\n",
    "    *   **`Name`**: Differentiates entities with the same role.\n",
    "    *   **`Metadata`**: Additional info, like timestamps or token usage.\n",
    "    *   **`Tool Calls`**: Requests from the model to use tools.\n",
    "\n",
    "### Conversation Structure\n",
    "\n",
    "*   A typical conversation consists of a structured sequence of messages, usually alternating between **`user`** and **`assistant`** messages.\n",
    "\n",
    "### LangChain Message Types\n",
    "\n",
    "*   LangChain represents messages as Python objects inheriting from `BaseMessage`.\n",
    "*   Key message types:\n",
    "    *   **`SystemMessage`**: Corresponds to the `system` role.\n",
    "    *   **`HumanMessage`**: Corresponds to the `user` role.\n",
    "    *   **`AIMessage`**: Corresponds to the `assistant` role.\n",
    "    *   **`AIMessageChunk`**: Used for streaming responses, also `assistant` role.\n",
    "    *   **`ToolMessage`**: Corresponds to the `tool` role.\n",
    "*   Other message types:\n",
    "    *   **`RemoveMessage`**: Manages chat history, not associated with any role.\n",
    "    *   **`FunctionMessage`**: Legacy message for the `function` role, use `ToolMessage` instead.\n",
    "\n",
    "### Specific Message Details\n",
    "\n",
    "*   **`SystemMessage`**:\n",
    "    *   Primes the AI model by providing context.\n",
    "    *   Implemented differently by providers (message role, API parameter, or no support).\n",
    "    *   LangChain adapts based on provider capabilities and tries to incorporate the content in a HumanMessage if system messages are not supported by the provider.\n",
    "*   **`HumanMessage`**:\n",
    "    *   Represents user input, usually text, sometimes multimodal content.\n",
    "    *   LangChain can convert a string input into a `HumanMessage`.\n",
    "*   **`AIMessage`**:\n",
    "    *   Model's response, may include text or tool calls.\n",
    "     *  The `content` property is not standardized and can be text or a list of dictionaries.\n",
    "    *   Includes attributes like `content`, `tool_calls`, `invalid_tool_calls`, `usage_metadata`, `id`, and `response_metadata`.\n",
    "*   **`AIMessageChunk`**:\n",
    "    *   For streaming responses.\n",
    "    *   Can be merged into a single `AIMessage` using the `+` operator.\n",
    "*   **`ToolMessage`**:\n",
    "    *   Contains the result of tool calls, with a `tool_call_id` and an `artifact` field.\n",
    "*   **`RemoveMessage`**: For managing chat history in LangGraph.\n",
    "*   **`(Legacy) FunctionMessage`**: For the legacy function-calling API. `ToolMessage` should be used instead.\n",
    "\n",
    "### OpenAI Format\n",
    "\n",
    "*   Chat models accept OpenAI's format as input, a dictionary with `role` and `content`.\n",
    "*   The output of models will be in terms of LangChain messages which can be converted to OpenAI format using `convert_to_openai_messages`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-24T19:24:47.002150976Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1612224118, 'load_duration': 1316112365, 'prompt_eval_count': 16, 'prompt_eval_duration': 52000000, 'eval_count': 33, 'eval_duration': 243000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-1db95c60-3523-4083-8bb0-1ef520f1b79c-0', usage_metadata={'input_tokens': 16, 'output_tokens': 33, 'total_tokens': 49})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.1')\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings or emotions like humans do. But thank you for asking! How can I assist you today? Do you have any questions or topics you'd like to discuss?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-24T19:24:47.365290728Z', 'done': True, 'done_reason': 'stop', 'total_duration': 357220106, 'load_duration': 12503855, 'prompt_eval_count': 16, 'prompt_eval_duration': 1000000, 'eval_count': 46, 'eval_duration': 342000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-2077083b-92b4-4eb8-a8f1-1124b2ada253-0', usage_metadata={'input_tokens': 16, 'output_tokens': 46, 'total_tokens': 62})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=' answer' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=' of' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=' course' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70'\n",
      "content=\"The answer, of course, depends on the time of day and atmospheric conditions.\\n\\n**During the Day:**\\nWhen the sun is overhead, the sky appears to be **blue**. This is because when sunlight enters Earth's atmosphere, it scatters in all directions due to interactions with tiny molecules of gases such as nitrogen and oxygen. The shorter (blue) wavelengths are scattered more than the longer (red) wavelengths, which is why we see a blue sky.\\n\\n**During Sunrise or Sunset:**\\nAs the sun rises or sets, the light has to travel through more of the Earth's atmosphere to reach our eyes, scattering off particles in the air. This scattering effect favors longer wavelengths like red and orange, making the sky appear **red**, **orange**, or **pink** during these times.\\n\\n**At Night:**\\nWhen it's dark outside, the sky appears **black**, as there is no sunlight to illuminate it. However, if you're in an area with minimal light pollution, you can see the stars and other celestial objects shining brightly against a dark blue or gray background.\\n\\nSo, to summarize:\\n\\n* Blue during the day\\n* Red/orange/pink during sunrise/sunset\\n* Black at night (with starry sky)\" additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2024-12-24T19:24:49.261091075Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1889358707, 'load_duration': 9285852, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 251, 'eval_duration': 1876000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-0994f30a-0747-45d9-8fcb-52b7444c6a70' usage_metadata={'input_tokens': 16, 'output_tokens': 251, 'total_tokens': 267}\n"
     ]
    }
   ],
   "source": [
    "messages = None\n",
    "for index, chunk in enumerate(model.stream([HumanMessage(\"what color is the sky?\")])):\n",
    "    if messages is None:\n",
    "        messages = chunk\n",
    "    else:\n",
    "        messages += chunk\n",
    "    if index > 5:\n",
    "        continue\n",
    "    print(chunk)\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage\n",
    "\n",
    "class SecurityProjectManager(HumanMessage):\n",
    "    def pretty_print(self):\n",
    "        print(f\"\\n{'='*30} {self.__class__.__name__} {'='*30}\\n\\n{self.content}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================\u001b[1m System Message \u001b[0m================================\n",
      "\n",
      "You are a helpful assistant.\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "Hello, how are you?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "I'm good, thank you! How can I assist you today?\n",
      "\n",
      "============================== SecurityProjectManager ==============================\n",
      "\n",
      "Can you tell me a joke?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Sure! Why don't scientists trust atoms? Because they make up everything!\n",
      "================================\u001b[1m Human Message \u001b[0m=================================\n",
      "\n",
      "That's funny! Can you help me with some math?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "Of course! What math problem do you need help with?\n",
      "\n",
      "============================== SecurityProjectManager ==============================\n",
      "\n",
      "What's the square root of 144?\n",
      "==================================\u001b[1m Ai Message \u001b[0m==================================\n",
      "\n",
      "The square root of 144 is 12.\n",
      "=================================\u001b[1m Tool Message \u001b[0m=================================\n",
      "\n",
      "Math tool used to calculate the square root of 144.\n"
     ]
    }
   ],
   "source": [
    "# Creating a sequence of messages with logical flow\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are a helpful assistant.\"),\n",
    "    HumanMessage(content=\"Hello, how are you?\"),\n",
    "    AIMessage(content=\"I'm good, thank you! How can I assist you today?\"),\n",
    "    SecurityProjectManager(content=\"Can you tell me a joke?\"),\n",
    "    AIMessage(content=\"Sure! Why don't scientists trust atoms? Because they make up everything!\"),\n",
    "    HumanMessage(content=\"That's funny! Can you help me with some math?\"),\n",
    "    AIMessage(content=\"Of course! What math problem do you need help with?\"),\n",
    "    SecurityProjectManager(content=\"What's the square root of 144?\"),\n",
    "    AIMessage(content=\"The square root of 144 is 12.\"),\n",
    "    ToolMessage(content=\"Math tool used to calculate the square root of 144.\", tool_call_id=\"12345\"),\n",
    "]\n",
    "\n",
    "for message in messages:\n",
    "    message.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'system'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages[0].type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
