{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Model Messages: An Overview\n",
    "\n",
    "This document details how messages are structured and used within chat models, particularly focusing on how LangChain handles them.\n",
    "\n",
    "### Core Concepts\n",
    "\n",
    "*   **Messages** are the basic units of communication in chat models, representing both inputs and outputs.\n",
    "*   Each message has a **`role`** and **`content`**, along with optional metadata.\n",
    "*   **LangChain** provides a consistent way to manage messages across different chat models.\n",
    "\n",
    "### Message Components\n",
    "\n",
    "*   **`Role`**:  This categorises the message and helps the chat model respond appropriately. Key roles include:\n",
    "    *   **`system`**: Used to instruct the model and provide context.\n",
    "    *   **`user`**: Represents input from a user.\n",
    "    *   **`assistant`**: Represents a response from the model.\n",
    "    *   **`tool`**: Passes results of a tool invocation back to the model.\n",
    "    *   `function`: A legacy role for OpenAI's function-calling API, `tool` should be used instead.\n",
    "*   **`Content`**: The actual message information, can be text or multimodal data (images, audio, video). Most models primarily use text.\n",
    "*   **Other Data**: Messages may include:\n",
    "    *   **`ID`**: A unique identifier.\n",
    "    *   **`Name`**: Differentiates entities with the same role.\n",
    "    *   **`Metadata`**: Additional info, like timestamps or token usage.\n",
    "    *   **`Tool Calls`**: Requests from the model to use tools.\n",
    "\n",
    "### Conversation Structure\n",
    "\n",
    "*   A typical conversation consists of a structured sequence of messages, usually alternating between **`user`** and **`assistant`** messages.\n",
    "\n",
    "### LangChain Message Types\n",
    "\n",
    "*   LangChain represents messages as Python objects inheriting from `BaseMessage`.\n",
    "*   Key message types:\n",
    "    *   **`SystemMessage`**: Corresponds to the `system` role.\n",
    "    *   **`HumanMessage`**: Corresponds to the `user` role.\n",
    "    *   **`AIMessage`**: Corresponds to the `assistant` role.\n",
    "    *   **`AIMessageChunk`**: Used for streaming responses, also `assistant` role.\n",
    "    *   **`ToolMessage`**: Corresponds to the `tool` role.\n",
    "*   Other message types:\n",
    "    *   **`RemoveMessage`**: Manages chat history, not associated with any role.\n",
    "    *   **`FunctionMessage`**: Legacy message for the `function` role, use `ToolMessage` instead.\n",
    "\n",
    "### Specific Message Details\n",
    "\n",
    "*   **`SystemMessage`**:\n",
    "    *   Primes the AI model by providing context.\n",
    "    *   Implemented differently by providers (message role, API parameter, or no support).\n",
    "    *   LangChain adapts based on provider capabilities and tries to incorporate the content in a HumanMessage if system messages are not supported by the provider.\n",
    "*   **`HumanMessage`**:\n",
    "    *   Represents user input, usually text, sometimes multimodal content.\n",
    "    *   LangChain can convert a string input into a `HumanMessage`.\n",
    "*   **`AIMessage`**:\n",
    "    *   Model's response, may include text or tool calls.\n",
    "     *  The `content` property is not standardized and can be text or a list of dictionaries.\n",
    "    *   Includes attributes like `content`, `tool_calls`, `invalid_tool_calls`, `usage_metadata`, `id`, and `response_metadata`.\n",
    "*   **`AIMessageChunk`**:\n",
    "    *   For streaming responses.\n",
    "    *   Can be merged into a single `AIMessage` using the `+` operator.\n",
    "*   **`ToolMessage`**:\n",
    "    *   Contains the result of tool calls, with a `tool_call_id` and an `artifact` field.\n",
    "*   **`RemoveMessage`**: For managing chat history in LangGraph.\n",
    "*   **`(Legacy) FunctionMessage`**: For the legacy function-calling API. `ToolMessage` should be used instead.\n",
    "\n",
    "### OpenAI Format\n",
    "\n",
    "*   Chat models accept OpenAI's format as input, a dictionary with `role` and `content`.\n",
    "*   The output of models will be in terms of LangChain messages which can be converted to OpenAI format using `convert_to_openai_messages`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings like humans do, but thank you for asking! I'm functioning properly and ready to help with any questions or tasks you may have. How about you? How's your day going so far?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T19:03:11.493834854Z', 'done': True, 'done_reason': 'stop', 'total_duration': 1758532857, 'load_duration': 1316866053, 'prompt_eval_count': 16, 'prompt_eval_duration': 55000000, 'eval_count': 53, 'eval_duration': 383000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-7858a9ff-fdc3-4c99-9de8-ca40115370ec-0', usage_metadata={'input_tokens': 16, 'output_tokens': 53, 'total_tokens': 69})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_ollama.chat_models import ChatOllama\n",
    "\n",
    "model = ChatOllama(model='llama3.1')\n",
    "\n",
    "model.invoke([HumanMessage(content=\"Hello, how are you?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I'm just a computer program, so I don't have feelings in the same way humans do. But thank you for asking! How can I assist you today?\", additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T19:03:23.760451212Z', 'done': True, 'done_reason': 'stop', 'total_duration': 278218954, 'load_duration': 20414852, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 34, 'eval_duration': 255000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-5b551356-d94b-4e0f-9179-5c9747aa4617-0', usage_metadata={'input_tokens': 16, 'output_tokens': 34, 'total_tokens': 50})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Hello, how are you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=' answer' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=' might' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=' seem' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=' simple' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=',' additional_kwargs={} response_metadata={} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b'\n",
      "content=\"The answer might seem simple, but it actually depends on the time of day and atmospheric conditions.\\n\\n**During the daytime:**\\nWhen the sun is out, the sky typically appears blue to our eyes. This is because the Earth's atmosphere scatters sunlight in all directions, with shorter (blue) wavelengths being scattered more than longer (red) wavelengths. This phenomenon is known as Rayleigh scattering, named after the British physicist Lord Rayleigh who first described it.\\n\\n**At sunrise and sunset:**\\nDuring these times, the sky can take on hues of red, orange, pink, or purple due to a different scattering process called Mie scattering. When sunlight passes through more of the Earth's atmosphere, the shorter blue wavelengths are scattered away, leaving mainly longer wavelengths like red and yellow to reach our eyes.\\n\\n**At night:**\\nWhen it's dark outside, the sky appears black because there is no direct sunlight to reflect off the atmosphere. However, if you're in a location with minimal light pollution, you might be able to see some stars or the Milky Way, which can appear as a faint glow on a dark blue background.\\n\\n**Other times:**\\nIn certain conditions, like during severe thunderstorms or when atmospheric particles are high (e.g., due to dust, smoke, or pollution), the sky can take on unusual colors, such as grayish-brown, greenish-gray, or even yellowish.\\n\\nSo, in summary, the color of the sky depends on the time of day and atmospheric conditions. But if you're looking for a simple answer, it's usually blue!\" additional_kwargs={} response_metadata={'model': 'llama3.1', 'created_at': '2024-12-17T19:06:16.161328315Z', 'done': True, 'done_reason': 'stop', 'total_duration': 2384630848, 'load_duration': 20667211, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 320, 'eval_duration': 2361000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)} id='run-74a34c4b-4c42-4a66-ba45-fe15108fcc5b' usage_metadata={'input_tokens': 16, 'output_tokens': 320, 'total_tokens': 336}\n"
     ]
    }
   ],
   "source": [
    "messages = None\n",
    "for index, chunk in enumerate(model.stream([HumanMessage(\"what color is the sky?\")])):\n",
    "    if messages is None:\n",
    "        messages = chunk\n",
    "    else:\n",
    "        messages += chunk\n",
    "    if index > 5:\n",
    "        continue\n",
    "    print(chunk)\n",
    "\n",
    "print(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
