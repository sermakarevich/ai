{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "*   **Multimodality** refers to the ability to work with data in different forms, such as text, audio, images, and video.\n",
    "*   Multimodality allows models and systems to process a mix of data types seamlessly.\n",
    "*   Multimodal support is relatively new and not yet standardised across model providers.\n",
    "*   LangChain's multimodal abstractions are designed to be flexible and accommodate different APIs.\n",
    "\n",
    "**Multimodality in Chat Models**\n",
    "\n",
    "*   Chat models could accept and generate multimodal inputs and outputs, handling various data types.\n",
    "*   To use multimodal models:\n",
    "    *   Identify which models support multimodality using the chat model integration table.\n",
    "    *   Reference the how-to guides for specific examples.\n",
    "\n",
    "*   **Inputs**:\n",
    "    *   Some models accept inputs like images, audio, video, or files.\n",
    "    *   The types of supported inputs depend on the model provider.\n",
    "    *   Most models that support multimodal inputs accept values in OpenAI's content blocks format, which is restricted to image inputs so far.\n",
    "    *   Models like Gemini also support native, model-specific representations for video and other byte inputs.\n",
    "    *   Multimodal inputs are passed using content blocks that specify a type and corresponding data.\n",
    "    *   The format of the content blocks may vary depending on the model provider.\n",
    "        *   For example, to pass an image to a chat model, you would specify the type as \"image_url\" and include the URL.\n",
    "        ```python\n",
    "        from langchain_core.messages import HumanMessage\n",
    "        message = HumanMessage(\n",
    "            content = [\n",
    "                {\"type\": \"text\", \"text\": \"describe the weather in this image\"},\n",
    "                {\"type\": \"image_url\", \"image_url\": {\"url\": image_url}},\n",
    "            ],\n",
    "        )\n",
    "        response = model.invoke([message])\n",
    "        ```\n",
    "*   **Outputs**:\n",
    "    *   Virtually no popular chat models support multimodal outputs, with the exception of OpenAI's `gpt-4o-audio-preview` model which can generate audio outputs.\n",
    "    *   Multimodal outputs will appear as part of the `AIMessage` response object.\n",
    "    *   The `ChatOpenAI` documentation has more information on how to use multimodal outputs.\n",
    "\n",
    "*  **Tools**:\n",
    "    *   No chat model is designed to work directly with multimodal data in a tool call request or `ToolMessage` result.\n",
    "    *   Models can interact with multimodal data by invoking tools with references (e.g., URLs) to the data.\n",
    "    *   Models can be equipped with tools to download and process images, audio, or video.\n",
    "\n",
    "**Multimodality in Embedding Models**\n",
    "\n",
    "*   **Embeddings** are vector representations of data used for tasks like similarity search and retrieval.\n",
    "*   The current embedding interface in LangChain is optimised for text-based data and will not work with multimodal data.\n",
    "*   The embedding interface is expected to expand to accommodate other data types like images, audio, and video as use cases become more common.\n",
    "\n",
    "**Multimodality in Vector Stores**\n",
    "\n",
    "*   Vector stores are databases for storing and retrieving embeddings, typically used in search and retrieval tasks.\n",
    "*   Vector stores are currently optimised for text-based data.\n",
    "*   The vector store interface is expected to expand to accommodate other data types like images, audio, and video as use cases become more common.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
