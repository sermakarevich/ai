{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LangChain's streaming capabilities are designed to enhance the user experience of applications built on Large Language Models (LLMs) by providing real-time feedback. Streaming is crucial for improving responsiveness due to the latency often associated with LLMs.\n",
    "\n",
    "Here's a breakdown of the key concepts:\n",
    "\n",
    "**What to Stream:**\n",
    "\n",
    "*   **LLM Outputs:** Streaming the output of the LLM itself is the most common use case, allowing users to see the text being generated incrementally.\n",
    "*   **Pipeline or Workflow Progress:** Streaming updates about the progress of workflows or pipelines provides users with a sense of the application's execution, including:\n",
    "    *   **LangGraph Workflows:** This involves tracking changes to the graph state as individual nodes request updates.\n",
    "    *   **LCEL Pipelines:** This involves capturing progress from individual sub-runnables as they execute.\n",
    "*  **Custom Data**: Custom data can be streamed from specific steps within a workflow (whether a tool or a LangGraph node), providing more granular insights into the execution of the process.\n",
    "\n",
    "**Streaming APIs:**\n",
    "\n",
    "*   LangChain offers two main APIs for streaming output in real-time, supported by components that implement the Runnable Interface.\n",
    "*   **`stream()` and `astream()`:**\n",
    "    *   These methods are used to stream outputs from individual Runnables (e.g., a chat model) or any workflow created with LangGraph.\n",
    "    *   `stream()` returns an iterator that yields chunks of output synchronously, while `astream()` is the asynchronous version for non-blocking workflows.\n",
    "    *   The type of chunk yielded depends on the component being streamed (e.g., `AIMessageChunk` for chat models).\n",
    "    *   When using these with LangGraph, you can control the type of output streamed using modes like \"values\", \"updates\", \"debug\", \"messages\", and \"custom\".\n",
    "    *  With LCEL, `stream()` and `astream()` will stream the output of the last step in the chain.\n",
    "*   **`astream_events`:**\n",
    "    *   This asynchronous API provides access to custom events and intermediate outputs from LLM applications built entirely with LCEL.\n",
    "    *   It is not usually needed with LangGraph, where `stream` and `astream` provide comprehensive capabilities.\n",
    "    *   `astream_events` returns an iterator that yields various types of events, allowing you to filter and process them.\n",
    "\n",
    "**Writing Custom Data to the Stream:**\n",
    "\n",
    "*   **LangGraph:** Use the `StreamWriter` to write custom data surfaced through `stream` and `astream`. This feature is not available for pure LCEL workflows.\n",
    "*   **LCEL:** Use `dispatch_events` or `adispatch_events` to write custom data surfaced through the `astream_events` API.\n",
    "\n",
    "**Auto-Streaming:**\n",
    "\n",
    "*   LangChain can automatically enable streaming mode in certain cases, even when youâ€™re not explicitly calling the streaming methods.\n",
    "*   When you call `invoke` (or `ainvoke`) on a chat model, LangChain will switch to streaming mode if it detects that you are trying to stream the overall application. This is done by using the `stream` or `astream` method.\n",
    "\n",
    "**Async Programming:**\n",
    "\n",
    "*   LangChain offers both synchronous and asynchronous versions of its methods, with async methods typically prefixed with \"a\" (e.g., `ainvoke`, `astream`).\n",
    "*   When writing async code, use async methods consistently for non-blocking behaviour and optimal performance.\n",
    "\n",
    "**Important Notes:**\n",
    "\n",
    "*   When processing chunks from a stream, ensure that the processing is efficient to avoid pausing the upstream component or causing timeouts.\n",
    "*   The legacy `astream_log` API is not recommended for new projects.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on_chain_start RunnableSequence {'input': {'topic': 'sun'}}\n",
      "on_prompt_start ChatPromptTemplate {'input': {'topic': 'sun'}}\n",
      "on_prompt_end ChatPromptTemplate {'output': ChatPromptValue(messages=[HumanMessage(content='tell me a joke about sun', additional_kwargs={}, response_metadata={})]), 'input': {'topic': 'sun'}}\n",
      "on_chat_model_start ChatOllama {'input': {'messages': [[HumanMessage(content='tell me a joke about sun', additional_kwargs={}, response_metadata={})]]}}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content='Why', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_start StrOutputParser {}\n",
      "on_parser_stream StrOutputParser {'chunk': 'Why'}\n",
      "on_chain_stream RunnableSequence {'chunk': 'Why'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' did', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' did'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' did'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' the', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' the'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' the'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' sun', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' sun'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' sun'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' go', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' go'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' go'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' to', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' to'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' to'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' therapy', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' therapy'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' therapy'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content='?\\n\\n', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': '?\\n\\n'}\n",
      "on_chain_stream RunnableSequence {'chunk': '?\\n\\n'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content='Because', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': 'Because'}\n",
      "on_chain_stream RunnableSequence {'chunk': 'Because'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' it', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' it'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' it'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' had', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' had'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' had'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' a', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' a'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' a'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' burning', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' burning'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' burning'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content=' issue', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': ' issue'}\n",
      "on_chain_stream RunnableSequence {'chunk': ' issue'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content='.', additional_kwargs={}, response_metadata={}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf')}\n",
      "on_parser_stream StrOutputParser {'chunk': '.'}\n",
      "on_chain_stream RunnableSequence {'chunk': '.'}\n",
      "on_chat_model_stream ChatOllama {'chunk': AIMessageChunk(content='', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-18T18:09:25.259034584Z', 'done': True, 'done_reason': 'stop', 'total_duration': 136843731, 'load_duration': 14709378, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 16, 'eval_duration': 119000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf', usage_metadata={'input_tokens': 16, 'output_tokens': 16, 'total_tokens': 32})}\n",
      "on_parser_stream StrOutputParser {'chunk': ''}\n",
      "on_chain_stream RunnableSequence {'chunk': ''}\n",
      "on_chat_model_end ChatOllama {'output': AIMessageChunk(content='Why did the sun go to therapy?\\n\\nBecause it had a burning issue.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-18T18:09:25.259034584Z', 'done': True, 'done_reason': 'stop', 'total_duration': 136843731, 'load_duration': 14709378, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 16, 'eval_duration': 119000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf', usage_metadata={'input_tokens': 16, 'output_tokens': 16, 'total_tokens': 32}), 'input': {'messages': [[HumanMessage(content='tell me a joke about sun', additional_kwargs={}, response_metadata={})]]}}\n",
      "on_parser_end StrOutputParser {'output': 'Why did the sun go to therapy?\\n\\nBecause it had a burning issue.', 'input': AIMessageChunk(content='Why did the sun go to therapy?\\n\\nBecause it had a burning issue.', additional_kwargs={}, response_metadata={'model': 'llama3.1', 'created_at': '2024-12-18T18:09:25.259034584Z', 'done': True, 'done_reason': 'stop', 'total_duration': 136843731, 'load_duration': 14709378, 'prompt_eval_count': 16, 'prompt_eval_duration': 2000000, 'eval_count': 16, 'eval_duration': 119000000, 'message': Message(role='assistant', content='', images=None, tool_calls=None)}, id='run-f7706db7-756a-47d0-b98a-78e8ecb7afaf', usage_metadata={'input_tokens': 16, 'output_tokens': 16, 'total_tokens': 32})}\n",
      "on_chain_end RunnableSequence {'output': 'Why did the sun go to therapy?\\n\\nBecause it had a burning issue.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "model = ChatOllama(model=\"llama3.1\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"tell me a joke about {topic}\")\n",
    "parser = StrOutputParser()\n",
    "chain = prompt | model | parser\n",
    "\n",
    "async for event in chain.astream_events({\"topic\": \"sun\"}, version=\"v2\"):\n",
    "    kind = event[\"event\"]\n",
    "    # if kind == \"on_chat_model_stream\":\n",
    "    #     print(event, end=\"|\", flush=True)\n",
    "    # else:\n",
    "    print(event[\"event\"], event[\"name\"], event['data'], flush=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
